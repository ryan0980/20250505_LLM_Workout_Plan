#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
short_qwen_xin.py

é’ˆå¯¹ Qwen2.5â€‘3B æ¨¡å‹çš„å‰ªæè„šæœ¬ï¼Œä½¿ç”¨æœ¬åœ° JSON æ•°ç»„æ ¼å¼çš„â€œpruneâ€æ–‡ä»¶å¤¹ï¼Œ
ä»…å–å‰ 200 æ¡æ ·æœ¬è®¡ç®— Block Influenceï¼ˆBIï¼‰ï¼Œå‰ªæ‰æœ€ä½ 20% çš„å±‚ã€‚
"""

import os
import glob
import json
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from torch.nn.functional import cosine_similarity

# â€”â€” 1. é…ç½®è·¯å¾„ & åˆ›å»ºè¾“å‡ºç›®å½• â€”â€” 
MODEL_PATH = "/root/autodl-tmp/model/q3bft_q"
OUTPUT_DIR = "/root/autodl-tmp/model/q3bft_q_p"
PRUNE_DIR  = "/root/autodl-tmp/datasets/lawbench3/valid_for_prune/prune"

os.makedirs(OUTPUT_DIR, exist_ok=True)

# â€”â€” 2. æ‰‹åŠ¨åŠ è½½æœ¬åœ° JSON æ–‡ä»¶ â€”â€” 
print("ğŸ” æ­£åœ¨åŠ è½½æœ¬åœ° prune æ–‡æœ¬â€¦")
all_texts = []
for json_path in glob.glob(os.path.join(PRUNE_DIR, "*.json")):
    with open(json_path, "r", encoding="utf-8") as f:
        records = json.load(f)
    for rec in records:
        all_texts.append(rec["question"])  # å¯æ ¹æ®éœ€è¦æ”¹ä¸º rec["text"] æˆ–æ‹¼æ¥å­—æ®µ

# ä»…å–å‰ 200 æ¡æ ·æœ¬
texts = all_texts[:200]
print(f"âœ… ä¸€å…±åŠ è½½åˆ° {len(all_texts)} æ¡æ–‡æœ¬ï¼Œå½“å‰ä»…ä½¿ç”¨å‰ {len(texts)} æ¡è¿›è¡Œ BI è®¡ç®—\n")

# â€”â€” 3. åŠ è½½æ¨¡å‹ & tokenizer â€”â€” 
print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼š{MODEL_PATH}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    torch_dtype=torch.float16
)
model.eval()
model.to("cuda")

# â€”â€” 4. è®¡ç®—å‰ªæå±‚æ•°ï¼ˆ20%ï¼‰ â€”â€” 
total_layers    = model.config.num_hidden_layers
prune_layer_num = max(1, int(total_layers * 0.20))
print(f"âš™ï¸ æ¨¡å‹å…±æœ‰ {total_layers} å±‚ï¼Œè®¡åˆ’å‰ªé™¤ {prune_layer_num} å±‚ï¼ˆçº¦ 20%ï¼‰ã€‚\n")

# â€”â€” 5. ç»Ÿè®¡æ¯å±‚çš„ä½™å¼¦ç›¸ä¼¼åº¦ â€”â€” 
layer_cos_sum = np.zeros(total_layers, dtype=np.float64)
token_count   = np.zeros(total_layers, dtype=np.int64)

print("ğŸ”¢ å¼€å§‹è®¡ç®— Block Influenceï¼ˆBIï¼‰åˆ†æ•°â€¦")
for idx, text in enumerate(texts, start=1):
    if not text or not text.strip():
        continue

    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=model.config.max_position_embeddings
    )
    input_ids      = inputs["input_ids"].to("cuda")
    attention_mask = inputs["attention_mask"].to("cuda")

    outputs       = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)
    hidden_states = outputs.hidden_states  # tuple é•¿åº¦ = total_layers+1

    for layer_idx in range(total_layers):
        h0 = hidden_states[layer_idx][0]
        h1 = hidden_states[layer_idx + 1][0]
        cos_vals = cosine_similarity(h0, h1, dim=-1)
        layer_cos_sum[layer_idx] += cos_vals.sum().item()
        token_count[layer_idx]   += cos_vals.numel()

    if idx % 50 == 0 or idx == len(texts):
        print(f"  å·²å¤„ç† {idx}/{len(texts)} ä¸ªæ ·æœ¬")

print("\nâœ… BI ç»Ÿè®¡å®Œæˆï¼Œå¼€å§‹è®¡ç®—æ¯å±‚å¾—åˆ†ï¼š")
bi_scores = []
for i in range(total_layers):
    avg_cos = (layer_cos_sum[i] / token_count[i]) if token_count[i] > 0 else 1.0
    bi = 1.0 - avg_cos
    bi_scores.append(bi)
    print(f"  å±‚ {i:02d} BI = {bi:.6f}")

# â€”â€” 6. æ’åº & å‰ªæ â€”â€” 
to_prune = sorted(range(total_layers), key=lambda i: bi_scores[i])[:prune_layer_num]
print(f"\nâœ‚ï¸ å°†å‰ªé™¤çš„å±‚ç´¢å¼•ï¼ˆLowest BIï¼‰ï¼š{to_prune}")

for idx in sorted(to_prune, reverse=True):
    del model.model.layers[idx]
model.config.num_hidden_layers = total_layers - prune_layer_num

# â€”â€” 7. ä¿å­˜å‰ªæåæ¨¡å‹ & tokenizer â€”â€” 
print(f"\nğŸ’¾ ä¿å­˜å‰ªæåæ¨¡å‹åˆ°ï¼š{OUTPUT_DIR}\n")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print("ğŸ‰ å‰ªæå®Œæˆï¼")
